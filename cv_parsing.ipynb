{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ehj5galrfha6",
        "outputId": "60c9bc2c-d425-4beb-db2b-61b43d23d284"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.7)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.9)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.5.0)\n",
            "Requirement already satisfied: pydantic-core==2.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.6.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.10)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.1.1)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install spacy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "print(nlp.vocab.vectors_length)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_EzSsWs2gDyj",
        "outputId": "6973551a-c5b7-4166-f2cf-91c6aba5e08d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install plac"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xiXXnEtbgoDJ",
        "outputId": "c63d4b20-c36b-43cb-ac54-014bc651a8bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting plac\n",
            "  Downloading plac-1.3.5-py2.py3-none-any.whl (22 kB)\n",
            "Installing collected packages: plac\n",
            "Successfully installed plac-1.3.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import unicode_literals, print_function\n",
        "import plac\n",
        "import random\n",
        "from pathlib import Path\n",
        "import spacy\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "340V5_RpgmHs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp.pipe_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zRhzEi46gs1P",
        "outputId": "feb5545d-a500-4823-d89f-024a14c17e84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import json\n",
        "# Access the JSON file\n",
        "json_file_path = '/content/drive/MyDrive/annotated_data_for_analysis_18th_august_2023.json'\n",
        "\n",
        "# Read the JSON file\n",
        "with open(json_file_path) as f:\n",
        "    data = json.load(f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2_sTH48ni1k",
        "outputId": "237421b4-8cf2-451b-ff3c-07cc24ca4c89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data=data[0:200]"
      ],
      "metadata": {
        "id": "-_xtWGsDno-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = None\n",
        "n_iter=100"
      ],
      "metadata": {
        "id": "aY--m7-PiI76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load the model\n",
        "if model is not None:\n",
        "    nlp = spacy.load(model)\n",
        "    print(\"Loaded model '%s'\" % model)\n",
        "else:\n",
        "    nlp = spacy.blank('en')\n",
        "    print(\"Created blank 'en' model\")\n",
        "\n",
        "# Set up the pipeline\n",
        "if 'ner' not in nlp.pipe_names:\n",
        "    ner = nlp.create_pipe('ner')\n",
        "    nlp.add_pipe('ner', last=True)  # Use the name 'ner' instead of the ner object\n",
        "else:\n",
        "    ner = nlp.get_pipe('ner')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SzfrdF5-iGaP",
        "outputId": "c3431780-f2a9-4a49-c31e-8a480cfa0742"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created blank 'en' model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp.pipe_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTTaPinjNxVh",
        "outputId": "3cee89d1-1c0e-4d16-cb09-bc61f18eb9cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ner']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.training.example import Example\n",
        "\n",
        "for _, annotations in data:\n",
        "    for ent in annotations.get('entities'):\n",
        "        ner.add_label(ent[2])\n",
        "\n",
        "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
        "with nlp.disable_pipes(*other_pipes):  # only train NER\n",
        "    optimizer = nlp.begin_training()\n",
        "    # Convert training data to Example objects\n",
        "    examples = []\n",
        "    for text, annotations in data:\n",
        "        doc = nlp.make_doc(text)\n",
        "        example = Example.from_dict(doc, annotations)\n",
        "        examples.append(example)\n",
        "\n",
        "        # Perform the update using Example objects\n",
        "        losses = {}\n",
        "        for example in tqdm(examples):\n",
        "            nlp.update([example], drop=0.5, losses=losses)\n",
        "\n",
        "        print(\"Training complete. Losses:\", losses)"
      ],
      "metadata": {
        "id": "m3COrc4SjhDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp.to_disk('nlp_model')"
      ],
      "metadata": {
        "id": "RMVPP6GTlzwb"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp_model = spacy.load('nlp_model')"
      ],
      "metadata": {
        "id": "4q3Pe0of5aJ1"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyMuPDF"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aqcPfxQk59_G",
        "outputId": "fae494e9-057a-4f4e-c27b-2ca07ffa4e0d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyMuPDF\n",
            "  Downloading PyMuPDF-1.23.1-cp310-none-manylinux2014_x86_64.whl (4.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting PyMuPDFb==1.23.0 (from PyMuPDF)\n",
            "  Downloading PyMuPDFb-1.23.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (26.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.3/26.3 MB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyMuPDFb, PyMuPDF\n",
            "Successfully installed PyMuPDF-1.23.1 PyMuPDFb-1.23.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz  # PyMuPDF\n",
        "\n",
        "pdf_file_path = \"/content/Anuj's Resume.pdf\"  # Replace with the actual path\n",
        "pdf_document = fitz.open(pdf_file_path)\n",
        "\n",
        "text = \"\"\n",
        "for page_number in range(pdf_document.page_count):\n",
        "    page = pdf_document[page_number]\n",
        "    text += page.get_text()\n",
        "pdf_document.close()\n",
        "print(text)"
      ],
      "metadata": {
        "id": "29MAjDRH6mR9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdcd786f-9941-4a8c-d957-08bbe6f1bd4f"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Anuj Vaghani\n",
            "anujvaghani0@gmail.com | 9328023350 | Surat, Gujarat, India\n",
            "LeetCode | GitHub | Linkedin | Portfolio\n",
            "Education\n",
            "Sarvajanik college of engineering and technology\n",
            "Surat\n",
            "Bachelor of Engineering Information Technology\n",
            "Augest 2019 - July 2023\n",
            "CGPA: 8.52\n",
            "Ashadeep Vidhyalay 4\n",
            "Surat\n",
            "CLASS 12 (GSEB) - SCIENCE, MATHEMATICS\n",
            "June 2017 - May 2019\n",
            "Percentage: 78.8%\n",
            "Experience\n",
            "Recruitment Smart | Data Scientist\n",
            "Remote | June 2023 - Present\n",
            " Experienced NLP Data Scientist with a strong focus on data annotation preprocessing. Pro\u001ccient in\n",
            "applying Natural Language Processing (NLP) techniques to optimize data quality for machine learning\n",
            "models. Skilled in developing and optimizing NLP algorithms, improving data annotation work\u001dows,\n",
            "and contributing to cutting-edge projects.\n",
            "Devotee Infotech Pvt. Ltd. | Data Scientist - Intern\n",
            "Surat | January 2023 - June 2023\n",
            " I was responsible for conducting data cleaning, modeling, and visualization tasks to improve data quality\n",
            "and accuracy. I also gained valuable experience working in a team-oriented, fast-paced environment. I\n",
            "also developed predictive models using machine learning algorithms such as regression and decision trees.\n",
            "Skills\n",
            "Programming Languages:\n",
            "Python, C, SQL, Java, Javascript\n",
            "Libraries/Frameworks:\n",
            "Flask, Pandas, NumPy, TensorFlow, NLTK, spaCy, Scikit-learn, Keras\n",
            "Tools / Platforms:\n",
            "Git, GitHub, Microsoft Azure, AWS, Google Cloud Platform\n",
            "Databases:\n",
            "MySQL, SQLite 3, MongoDB\n",
            "Projects / Open-Source\n",
            "Coccidiosis-Diseases-class\u001ccation | Link\n",
            "Python, Deep Learning, Docker, Tensor\u001dow\n",
            " In this project, I developed a deep learning model using the Keras API in TensorFlow to classify\n",
            "Coccidiosis disease based on microscopic images of a\u001bected cells. Coccidiosis is a parasitic disease that\n",
            "a\u001bects animals, particularly poultry, and early detection is crucial for e\u001bective treatment. Leveraging\n",
            "deep learning techniques, I aimed to provide a reliable and automated system for disease identi\u001ccation,\n",
            "aiding in timely interventions and reducing the impact on livestock.\n",
            "Text-Summarization-NLP-Project | Link\n",
            "Python, NLP, \u001daskApi, Docker, Tensor\u001dow\n",
            "The Text Summarization NLP Project aims to create an advanced summarization system capable of\n",
            "e\u001eciently processing diverse textual content such as articles, news reports, and research papers. By\n",
            "leveraging NLP techniques, the system automatically generates concise and coherent summaries,\n",
            "e\u001bectively alleviating information overload and enhancing content comprehension for users.\n",
            "Online Retail Customer Segmentation | Link\n",
            "Python, NumPy, Pandas, Scikit-learn, and Seaborn\n",
            " I contributed to a project on Online Retail Customer Segmentation, which involved utilizing machine\n",
            "learning techniques to segment customers based on their purchasing behavior. The aim of the project\n",
            "was to provide insights that would inform marketing strategies and improve customer engagement.\n",
            " I implemented two unsupervised learning algorithms for customer segmentation, K-Means clustering,\n",
            "and Hierarchical Agglomerative Clustering. I evaluated the performance of the models using the\n",
            "Silhouette score, which measures how well the data points \u001ct into their assigned clusters.\n",
            "Airline Passenger Referral Prediction | Link\n",
            "Python, NumPy, Pandas, Scikit-learn, and Seaborn\n",
            " Airline passenger referral prediction refers to the task of predicting whether a passenger will refer other\n",
            "people to \u001dy with the same airline. This can be a valuable prediction for airlines to make, as\n",
            "word-of-mouth referrals can be a powerful marketing tool and can help airlines to attract new customers.\n",
            " I applied several machine learning algorithms, including logistic regression, random forest, and support\n",
            "vector machines, to predict the likelihood of a passenger becoming a referral.\n",
            "Bike Sharing Demand Prediction | Link\n",
            "Python, NumPy, Pandas, Random Forest Regression, Seaborn\n",
            " The goal of bike-sharing-demand-prediction is to create accurate and reliable models that can forecast\n",
            "the demand for bike-sharing services based on various factors, such as time of day, day of week, weather\n",
            "conditions, and location. These models can be used by bike-sharing companies to optimize their\n",
            "operations, such as determining the number of bikes to make available at di\u001berent locations, adjusting\n",
            "pricing and promotions, and improving customer satisfaction.\n",
            " The model utilized a combination of regression algorithms, feature engineering, and ensemble methods\n",
            "to capture the complex relationships between the predictor variables and the bike rental demand.\n",
            "Certifications\n",
            " Breast Cancer Detection with Machine published - IJRASET (International Journal for Research in\n",
            "Applied Science and Engineering Technology)\n",
            " Microsoft Certi\u001ced: Azure Data Fundamentals (DP-900) - Microsoft\n",
            " Microsoft Certi\u001ced: Azure AI Fundamentals (AI-900) - Microsoft\n",
            " Financial Fraud detection Approaches with ML - GTU International Conference\n",
            "Honors & Awards\n",
            " Achieved Ultimate Milestone in Google Cloud Ready Facilitator and 30 Days of Google Cloud Program.\n",
            " Solved over 600 problems on LeetCode, honing problem-solving skills and developing pro\u001cciency in\n",
            "algorithms and data structures.\n",
            " Contributed to Hacktoberfest program by submitting high- quality pull requests.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp_model(text)\n",
        "for ent in doc.ents:\n",
        "    print(f'{ent.label_.upper():{30}}- {ent.text}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xM7FuYVqAEyK",
        "outputId": "9803553e-35cd-451b-d9c0-774ff74f9055"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EMAIL_ID                      - anujvaghani0@gmail.com\n",
            "MOBILE_NO                     - 9328023350\n",
            "NAME                          - | Linkedin\n",
            "COLLEGE                       - Sarvajanik college of engineering and technology\n",
            "DEGREE                        - Bachelor of Engineering\n",
            "NAME                          - Ashadeep Vidhyalay\n",
            "PAST_COMPANY_PROJECT_DETAIL   - Recruitment Smart | Data Scientist\n",
            "NAME                          -  Experienced\n",
            "PAST_DESIGNATION              - NLP Data Scientist\n",
            "PAST_DESIGNATION              - Data Scientist\n",
            "CURRENT_COMPANY_HARD_SKILLS_USED_(TECHNICAL_SKILLS)- Python\n",
            "CURRENT_COMPANY_HARD_SKILLS_USED_(TECHNICAL_SKILLS)- C\n",
            "CURRENT_COMPANY_HARD_SKILLS_USED_(TECHNICAL_SKILLS)- SQL\n",
            "CURRENT_COMPANY_HARD_SKILLS_USED_(TECHNICAL_SKILLS)- Java\n",
            "CURRENT_COMPANY_HARD_SKILLS_USED_(TECHNICAL_SKILLS)- AWS\n",
            "CURRENT_COMPANY_HARD_SKILLS_USED_(TECHNICAL_SKILLS)- Python\n",
            "CURRENT_COMPANY_HARD_SKILLS_USED_(TECHNICAL_SKILLS)- NLP\n",
            "SPECIALISATION                - Applied Science and Engineering Technology\n",
            "NAME                          -  Microsoft\n",
            "NAME                          -  Microsoft Certi\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r nlp_model.zip nlp_model\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EENT4he2Ail_",
        "outputId": "ae9ac99f-15bd-41d6-dcfb-e665db023de4"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: nlp_model/ (stored 0%)\n",
            "  adding: nlp_model/meta.json (deflated 69%)\n",
            "  adding: nlp_model/config.cfg (deflated 59%)\n",
            "  adding: nlp_model/vocab/ (stored 0%)\n",
            "  adding: nlp_model/vocab/lookups.bin (stored 0%)\n",
            "  adding: nlp_model/vocab/vectors.cfg (stored 0%)\n",
            "  adding: nlp_model/vocab/strings.json (deflated 73%)\n",
            "  adding: nlp_model/vocab/key2row (stored 0%)\n",
            "  adding: nlp_model/vocab/vectors (deflated 45%)\n",
            "  adding: nlp_model/tokenizer (deflated 81%)\n",
            "  adding: nlp_model/ner/ (stored 0%)\n",
            "  adding: nlp_model/ner/moves (deflated 89%)\n",
            "  adding: nlp_model/ner/cfg (deflated 33%)\n",
            "  adding: nlp_model/ner/model (deflated 7%)\n"
          ]
        }
      ]
    }
  ]
}